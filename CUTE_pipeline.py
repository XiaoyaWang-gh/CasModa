import os
import json
import subprocess
import sys
import time
import logging
import random

from dotenv import load_dotenv

from CUTE_components.models import Prefix_datapoint, Failed_To_Be_Repair_datapoint, Oracle_datapoint, Test_Unit_Status, Result_datapoint
from mytemplate.bf_repair_template import compile_failed_repair_template
from mytemplate.tf_repair_template import test_failed_repair_template
from chatgpt_api.sequential_chatgpt_api_proxy import generate_chat_completion
from CUTE_components.dataset import Prefix_Dataset, Oracle_Dataset
from CUTE_components.generate_stage1 import bm25_retrived_demos4prefix, get_prefix_prompt
from CUTE_components.generate_stage2 import bm25_retrived_demos4oracle, get_oracle_prompt
from CUTE_components.validate import find_objects_with_id, find_objects_by_pro_with_ver
from CUTE_components.repair import extract_stderr_message, extract_stdout_message
from util.utils import write_to_file
from util.strUtils import fetch_method_chatgpt_out, hascode, form_complete_statement, make_classpath
from util.use_unixcoder import prefix_diversity_retriever, oracle_diversity_retriever, read_unixcoder
from util.decorators import timer

sys.stdout.reconfigure(encoding='utf-8')
sys.stderr.reconfigure(encoding='utf-8')

logger = logging.getLogger(__name__)

message_list = [
    {"role": "system", "content": "You are a proficient and helpful assistant in java testing with JUnit4 framework."}
]

def clear_chat_history():
    '''
    é‡ç½®å’Œchatgptçš„å†å²èŠå¤©è®°å½•
    '''
    global message_list

    message_list = [
        {"role": "system", "content": "You are a proficient and helpful assistant in java testing with JUnit framework."}]


def call_chatgpt(prompt_str):
    # æé—®ç¯èŠ‚
    one_question = {"role": "user", "content": prompt_str}
    message_list.append(one_question)
    answer = generate_chat_completion(message_list)
    # å–„åç¯èŠ‚
    one_talk = {"role": "assistant", "content": answer}
    message_list.append(one_talk)
    # è¿”å›ç»“æœ
    return answer


def encapsulate_into_test(testbody: str, testname: str) -> str:
    return f"public void {testname} {{\n{testbody}\n}}"


# ç”±äºä¸æ˜¯mainå‡½æ•°è°ƒç”¨ï¼Œæ‰€ä»¥è¦æ‰‹åŠ¨ç»™ç¯å¢ƒå˜é‡èµ‹å€¼
ID_PLACEHOLDER = "<ID>"
TM_PLACEHOLDER = "<TestMethodPlaceHolder>"

# ç»™è·¯å¾„å ä½ç¬¦èµ‹å€¼
pro = "binance"
# for swap demo pool
demo_pro_dict = {
    "csv":"gson",
    "gson":"csv",
    "chart":"lang",
    "lang":"chart",
    "binance":"binance"
}
res_type = "nonstop"
shot_num = 5

# å‡†å¤‡æ•°æ®è·¯å¾„
prefix_queryset_dir_path = f"txt_repo/prefix/query_set/{pro}"
# prefix_demopool_dir_path = f"txt_repo/prefix/demo_pool/{pro[:-5]}" # å»æ‰åé¢çš„_plus
prefix_demopool_dir_path = f"txt_repo/prefix/demo_pool/{demo_pro_dict[pro]}"
oracle_demopool_dir_path = f"txt_repo/oracle/demo_pool/{demo_pro_dict[pro]}"

testClass_place_path = f"txt_repo/validation/{pro}/testClass_place.json"
testClass_dir_path = f"txt_repo/validation/{pro}/test_class_content"
# id_to_num_file = f"txt_repo/validation/{pro[:-5]}/id_to_num.txt"
common_project_to_path = "txt_repo/validation/common/pro_with_ver_to_path.json"

testunit_compile_stderr_dir_path = f"txt_repo/validation/{pro}/output/{res_type}/testunit_compile"
testunit_execute_stdout_dir_path = f"txt_repo/validation/{pro}/output/{res_type}/testunit_execute"


overall_result_file = f"txt_repo/validation/{pro}/output/{res_type}/overall_result.txt"
passed_unit_file = f"txt_repo/validation/{pro}/output/{res_type}/passed_test_unit.txt"
check_chatgpt_file = f"txt_repo/validation/{pro}/output/{res_type}/check_chatgpt.txt"
original_chatgpt_out_file = f"txt_repo/validation/{pro}/output/{res_type}/original_chatgpt_out.txt"
chat_history_file = f"txt_repo/validation/{pro}/output/{res_type}/chat_history.txt"


def compile_mytest(proName: str, test: str, id: int, err_save_path: str) -> tuple[bool, str]:
    # ç¡®å®šä¸‹å»å“ªæ‰¾Test.txtæ–‡ä»¶
    with open(testClass_place_path, "r") as f:
        testClass_info = json.load(f)
    testClass_info_json_list = [dict(item) for item in testClass_info]
    pro_with_ver, test_class_name = find_objects_with_id(
        id, testClass_info_json_list)
    test_class_shell_path = os.path.join(
        testClass_dir_path, test_class_name+".txt")
    with open(test_class_shell_path, "r") as f:
        test_class_shell = f.read()
    tmp_test_class = test_class_shell.replace(TM_PLACEHOLDER, test)
    final_test_class = tmp_test_class.replace(
        ID_PLACEHOLDER, str(id))  # è‡³æ­¤å¾—åˆ°äº†å¯ä»¥ç”¨äºç¼–è¯‘çš„æµ‹è¯•ç±»
    # å°†æµ‹è¯•ç±»ä¿å­˜åˆ°ç”¨äºç¼–è¯‘çš„ä½ç½®test_class_save_path
    with open(common_project_to_path, "r") as f:
        common_info = json.load(f)
    common_info_json_list = [dict(item) for item in common_info]
    focal_class_path, test_class_path, middle_path = find_objects_by_pro_with_ver(
        proName, pro_with_ver, common_info_json_list)
    test_class_save_path = test_class_path
    os.makedirs(test_class_save_path, exist_ok=True)
    test_class_save_name = test_class_name+str(id)+".java"
    write_to_file(test_class_save_path+"/" +
                  test_class_save_name, final_test_class, "w")
    # ç¼–è¯‘
    libpath = "C:\\Libraries\\maven-3.5.0\\lib\\*"
    binance_cp_1 = "C:\\dataset\\binance-connector-java\\target\\classes"
    binance_cp_2 = "C:\\dataset\\binance-connector-java\\target\\test-classes"
    # ç›´æ¥å­—ç¬¦ä¸²ç›¸åŠ è¿‡äºugly
    # classpath = make_classpath(focal_class_path, middle_path,
    #                            test_class_path, libpath)
    classpath = make_classpath(libpath,binance_cp_1,binance_cp_2)
    compile_command = "javac -J-Duser.language=en -cp " + classpath + \
        test_class_save_path+"\\"+test_class_save_name + " -d "+binance_cp_2
    compile_result = subprocess.run(compile_command, shell=True,
                                    capture_output=True, text=True)
    # åœ¨è¿”å›ç¼–è¯‘ç¼–è¯‘ç»“æœä¹‹å‰ï¼Œå…ˆæŠŠåˆšæ‰çš„ç±»åˆ äº†
    os.remove(test_class_save_path+"/" +
              test_class_save_name)  # è°ƒè¯•æ—¶ç•™ä¸‹ï¼Œè¿è¡Œæ—¶ç…§æ—§remove
    if compile_result.returncode == 0:  # ç¼–è¯‘æˆåŠŸ
        # åªæœ‰ç¼–è¯‘æˆåŠŸæ‰æœ‰.classæ–‡ä»¶å¯åˆ 
        os.remove(binance_cp_2+"\\unit\\" +
                  test_class_save_name.replace(".java", ".class"))
        return True, None
    else:  # ç¼–è¯‘å¤±è´¥ å°†stderrä¿å­˜ä¸‹æ¥å¹¶è¿”å›
        stderr_filename = test_class_name+str(id)+"_stderr.txt"
        stderr_filename_save_path = os.path.join( err_save_path, stderr_filename)
        # å¦‚æœç¼–è¯‘å¤±è´¥ä¸‰æ¬¡ï¼Œé‚£ä¹ˆä¹ŸæŠŠä¸‰æ¬¡çš„ç»“æœå†™åœ¨ä¸€èµ·
        # æŠŠç¼–è¯‘å¤±è´¥çš„å‘½ä»¤å†™ä¸‹ï¼Œæ–¹ä¾¿è°ƒè¯•
        print(f"[1]compile_command : \n{compile_command}\n")
        write_to_file(stderr_filename_save_path, compile_command+"\n\n")
        print(f"[2]compile_result.returncode : \n{compile_result.returncode}\n")
        original_err = compile_result.stderr
        err_wo_waring = "\n".join([line for line in original_err.split('\n') if not line.startswith("warning")])
        print(f"[3]compile_result.stderr : \n{err_wo_waring}\n")
        print(f"[4]compile_result.stdout : \n{compile_result.stdout}\n")
        
        write_to_file(stderr_filename_save_path, err_wo_waring+"\n\n")
        
        return False, err_wo_waring


def execute_mytest(proName: str, test: str, id: int, out_save_path: str) -> tuple[bool, str]:
    # ç¡®å®šä¸‹å»å“ªæ‰¾Test.txtæ–‡ä»¶
    with open(testClass_place_path, "r") as f:
        testClass_info = json.load(f)
    testClass_info_json_list = [dict(item) for item in testClass_info]
    pro_with_ver, test_class_name = find_objects_with_id(
        id, testClass_info_json_list)
    test_class_shell_path = os.path.join(
        testClass_dir_path, test_class_name+".txt")
    with open(test_class_shell_path, "r") as f:
        test_class_shell = f.read()
    tmp_test_class = test_class_shell.replace(TM_PLACEHOLDER, test)
    final_test_class = tmp_test_class.replace(
        ID_PLACEHOLDER, str(id))  # è‡³æ­¤å¾—åˆ°äº†å¯ä»¥ç”¨äºç¼–è¯‘çš„æµ‹è¯•ç±»
    # å°†æµ‹è¯•ç±»ä¿å­˜åˆ°ç”¨äºç¼–è¯‘çš„ä½ç½®test_class_save_path
    with open(common_project_to_path, "r") as f:
        common_info = json.load(f)
    common_info_json_list = [dict(item) for item in common_info]
    focal_class_path, test_class_path, middle_path = find_objects_by_pro_with_ver(
        proName, pro_with_ver, common_info_json_list)
    test_class_save_path = test_class_path
    os.makedirs(test_class_save_path, exist_ok=True)
    test_class_save_name = test_class_name+str(id)+".java"
    write_to_file(test_class_save_path+"/" +
                  test_class_save_name, final_test_class, "w")
    # ç¼–è¯‘
    libpath = "C:\\Libraries\\maven-3.5.0\\lib\\*"
    binance_cp_1 = "C:\\dataset\\binance-connector-java\\target\\classes"
    binance_cp_2 = "C:\\dataset\\binance-connector-java\\target\\test-classes"
    classpath = make_classpath(libpath,binance_cp_1,binance_cp_2)
    # classpath = make_classpath(
    #     focal_class_path, middle_path, test_class_path, middle_path+"\\generated_by_chatgpt", libpath)  # ç›´æ¥å­—ç¬¦ä¸²ç›¸åŠ è¿‡äºugly

    compile_command = "javac -J-Duser.language=en -cp " + classpath + \
        test_class_save_path+"\\"+test_class_save_name+ " -d "+binance_cp_2

    compile_result = subprocess.run(compile_command, shell=True,
                                    capture_output=True, text=True)
    # åˆ°è¿™ä¸€å¥ä»¥å‰ï¼Œéƒ½å’Œcompile_mytestä¸€æ ·
    if compile_result.returncode != 0:
        os.remove(test_class_save_path+"/"+test_class_save_name)
        original_err = compile_result.stderr
        err_wo_waring = "\n".join([line for line in original_err.split('\n') if not line.startswith("warning")])
        return False, err_wo_waring  # å¦‚æœç¼–è¯‘å¤±è´¥ï¼Œè¯´æ˜ä¸æ˜¯ç¬¬ä¸€æ¬¡è¿›å…¥æ‰§è¡Œ
    # æ‰§è¡Œæµ‹è¯•ç±»
    execute_command = "java -cp " + classpath + " org.junit.runner.JUnitCore " + \
        "unit."+ test_class_name+str(id)

    test_result = subprocess.run(
        execute_command, shell=True, capture_output=True, text=True)
    if test_result.returncode == 0:  # æ‰§è¡ŒæˆåŠŸ
        return True, None
    else:  # æ‰§è¡Œå¤±è´¥ å°†stdoutä¿å­˜ä¸‹æ¥å¹¶è¿”å›
        stdout_filename = test_class_name+str(id)+"_stdout.txt"
        stdout_filename_save_path = os.path.join(
            out_save_path, stdout_filename)
        # å¦‚æœæ‰§è¡Œå¤±è´¥2æ¬¡ï¼Œé‚£ä¹ˆä¹ŸæŠŠ2æ¬¡çš„ç»“æœå†™åœ¨ä¸€èµ·
        # æŠŠæ‰§è¡Œå¤±è´¥çš„å‘½ä»¤å†™ä¸‹ï¼Œæ–¹ä¾¿è°ƒè¯•
        print(f"[1]execute_command : \n{execute_command}\n")
        print(f"[2]test_result.returncode : \n{test_result.returncode}\n")
        print(f"[3]test_result.stderr : \n{test_result.stderr}\n")
        print(f"[4]test_result.stdout : \n{test_result.stdout}\n")
        write_to_file(stdout_filename_save_path, execute_command+"\n\n")
        write_to_file(stdout_filename_save_path, test_result.stdout+"\n\n")
        return False, test_result.stdout


def repair_bf(test_with_only_prefix: str, compile_out: str, query: Prefix_datapoint, chatgpt_max_num: int) -> str:
    # ä»compile_outä¸­æå–å‡ºæœ‰æ•ˆéƒ¨åˆ†
    useful_compile_out = extract_stderr_message(compile_out)
    assert useful_compile_out is not None
    datapoint = Failed_To_Be_Repair_datapoint(
        query.classname, query.focalname_paralist, useful_compile_out, test_with_only_prefix)
    # æ„å»ºprompt
    bf_repair_prompt = compile_failed_repair_template(datapoint)

    has_code_flg = False
    for i in range(1, chatgpt_max_num, 1):  # retryå¤–åŒ…çŠ¶æ€ç çš„é—®é¢˜ï¼Œè¿™é‡Œè§£å†³ç©ºå€¼å’Œä¸å«ä»£ç çš„é—®é¢˜
        write_to_file(check_chatgpt_file, f"ç¬¬{i}æ¬¡è°ƒç”¨chatgpt (ç”¨äºä¿®å¤ç¼–è¯‘é”™è¯¯)\n")
        chatgpt_response = call_chatgpt(bf_repair_prompt)
        # time.sleep(6)
        if hascode(chatgpt_response):  # å¦‚æœè°ƒç”¨è¿”å›ä¸­åŒ…å«ä»£ç å—(è€Œä¸ä»…ä»…æ˜¯æŠ±æ­‰)ï¼Œå°±è·³å‡ºå¾ªç¯
            has_code_flg = True
            write_to_file(original_chatgpt_out_file,
                          chatgpt_response+"\n\n" + '*'*150+'\n')
            break
    if not has_code_flg:
        logger.warning(f"repair_bf -- has_code_flg : {has_code_flg}")
        return ""
    # è¾“å‡ºå¥‡å¥‡æ€ªæ€ªï¼Œè·å–æœ‰ç”¨çš„ä»£ç å—
    code_block = fetch_method_chatgpt_out(chatgpt_response)
    return code_block


def repair_tf_test_unit(test_unit: str, execute_out: str, query: Prefix_datapoint, chatgpt_max_num: int) -> str:
    # ä»execute_outä¸­æå–å‡ºæœ‰æ•ˆéƒ¨åˆ†
    useful_execute_out = extract_stdout_message(execute_out)
    assert useful_execute_out is not None
    datapoint = Failed_To_Be_Repair_datapoint(
        query.classname, query.focalname_paralist, useful_execute_out, test_unit)
    # æ„å»ºprompt
    tf_repair_prompt = test_failed_repair_template(datapoint)

    # ä»ä¸‹é¢å¼€å§‹å’Œrepair_bfä¸€æ ·
    has_code_flg = False
    for i in range(1, chatgpt_max_num, 1):  # retryå¤–åŒ…çŠ¶æ€ç çš„é—®é¢˜ï¼Œè¿™é‡Œè§£å†³ç©ºå€¼å’Œä¸å«ä»£ç çš„é—®é¢˜
        write_to_file(check_chatgpt_file, f"ç¬¬{i}æ¬¡è°ƒç”¨chatgpt ç”¨äºä¿®å¤æ‰§è¡Œé”™è¯¯\n")
        chatgpt_response = call_chatgpt(tf_repair_prompt)
        # time.sleep(6)
        if hascode(chatgpt_response):  # å¦‚æœè°ƒç”¨è¿”å›ä¸­åŒ…å«ä»£ç å—(è€Œä¸ä»…ä»…æ˜¯æŠ±æ­‰)ï¼Œå°±è·³å‡ºå¾ªç¯
            has_code_flg = True
            write_to_file(original_chatgpt_out_file,
                          chatgpt_response+"\n\n" + '*'*150+'\n')
            break
    if not has_code_flg:
        logger.warning(f"repair_tf -- has_code_flg : {has_code_flg}")
        return ""
    # è¾“å‡ºå¥‡å¥‡æ€ªæ€ªï¼Œè·å–æœ‰ç”¨çš„ä»£ç å—
    code_block = fetch_method_chatgpt_out(chatgpt_response)
    return code_block

@timer
def main():

    # è®¾ç½®ä¸€ä¸ªloggerï¼Œå–ä»£å¯¼å‡ºæ•£è½çš„print
    logger = logging.getLogger(__name__)
    logger.setLevel(level=logging.INFO)
    handler = logging.StreamHandler()
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    handler.setFormatter(formatter)
    logger.addHandler(handler)

    load_dotenv()
    # è¯»å–ç¯å¢ƒå˜é‡
    querynum_dict_str = os.getenv("QUERYNUM_DICT")
    querynum_dict = json.loads(querynum_dict_str)
    # testunit_compile_num = int(os.getenv("TESTUNIT_COMPILE_NUM"))
    # testunit_execute_num = int(os.getenv("TESTUNIT_EXECUTE_NUM"))

    chatgpt_max_num = int(os.getenv("CHATGPT_MAX_NUM"))
    sample_order = os.getenv("SAMPLE_ORDER")
    ORACLE_PLACEHOLDER = os.getenv("ORACLE_PLACEHOLDER")
    selector = os.getenv("SELECTOR")
    logger.info(f"selector : {selector}")

    # sample_idx_lst_dict_str = os.getenv("NEW_SAMPLE_IDX_LST_DICT")
    # sample_idx_lst_dict = json.loads(sample_idx_lst_dict_str)
    # picked_list = sample_idx_lst_dict[pro]

    prefix_queryset = Prefix_Dataset(
        prefix_queryset_dir_path,
        "constr_sign.txt",
        "fake_test_input.txt",
        "classname.txt",
        "focalname_paralist.txt",
        "test_name.txt"
    )
    prefix_queryset_list = prefix_queryset.parse()
    assert len(prefix_queryset_list) == querynum_dict[pro]

    prefix_demopool = Prefix_Dataset(
        prefix_demopool_dir_path,
        "constr_sign.txt",
        "test_input.txt",
        "classname.txt",
        "focalname_paralist.txt",
        "test_name.txt"
    )

    if selector == "unixcoder":
        prefix_unixcoder_tensor_file = os.path.join(
            prefix_queryset_dir_path, "unixcoder_tensor.txt")  # åªæœ‰ç”¨äº†unixcoderæ‰è¦è¿™å¥
        
        prefix_unixcoder_tensor_list = read_unixcoder(
            prefix_unixcoder_tensor_file)  # åªæœ‰ç”¨äº†unixcoderæ‰è¦è¿™å¥

    # m_n_tuple_str = os.getenv("N_M")
    # parsed_list = eval(m_n_tuple_str)  # è§£æå­—ç¬¦ä¸²ä¸ºPythonå¯¹è±¡
    # m_n_tuple_list = [tuple(item) for item in parsed_list]
    # for m_n_tuple in m_n_tuple_list:
    testunit_compile_num, testunit_execute_num = 4,3 # m_n_tuple

    
    result_datapoint_list = []
    # è¿›å…¥å¾ªç¯å•¦
    start_idx = 1  # 1
    end_idx = 100  # len(prefix_queryset_list)
    aborted_num = 0
    
    bf_num, tf_num, passed_num = 0, 0, 0

    for id in range(start_idx, end_idx+1, 1):
        try:
            # for id in [8]:
            logger.info(f"ğŸ’«æ¥åˆ°ç¬¬{id}ä¸ªè¾“å…¥ï¼Œè¿˜å‰©{len(prefix_queryset_list)-id}ä¸ªğŸ’«")

            result_datapoint = Result_datapoint(
                id, 0, 0, Test_Unit_Status.NONE_STATUS)

            tmp_query = prefix_queryset_list[id-1]
            
            now_prefix_demopool_list = prefix_demopool.parse()
            # ä¸åŒæ£€ç´¢å™¨èµ°ä¸åŒåˆ†æ”¯
            if selector == "unixcoder":
                
                tmp_query_tensor = prefix_unixcoder_tensor_list[id-1]
                prefix_candidate_demos,demo_sim_list = prefix_diversity_retriever(
                    tmp_query_tensor, now_prefix_demopool_list, shot_num)
                
                if sample_order == "ascending":
                    sorted_prefix_candidate_demos = [
                        tu[1] for tu in sorted(
                            enumerate(prefix_candidate_demos),key=lambda tu:demo_sim_list[tu[0]],reverse=False
                        )
                    ]
                elif sample_order == "descending":
                    sorted_prefix_candidate_demos = [
                        tu[1] for tu in sorted(
                            enumerate(prefix_candidate_demos),key=lambda tu:demo_sim_list[tu[0]],reverse=True
                        )
                    ]
            elif selector == "real_random":
                prefix_candidate_demos = random.sample(
                    now_prefix_demopool_list, shot_num)
            
                    
            # if selector == "bm25":
            #     now_prefix_demopool_list = prefix_demopool.parse(
            #         query_id=tmp_query.classname+tmp_query.focalname_paralist)
            #     prefix_candidate_demos = bm25_retrived_demos4prefix(
            #         tmp_query, now_prefix_demopool_list, sample_order, shot_num)

            if sample_order == "ascending" or sample_order == "descending":
                prefix_gen_prompt = get_prefix_prompt(
                    sorted_prefix_candidate_demos, tmp_query)
            else:
                prefix_gen_prompt = get_prefix_prompt(
                    prefix_candidate_demos, tmp_query)

            logger.info('-'*100)
            # ç”Ÿæˆprefix
            # write_to_file(chat_history_file, ">>1<< \n")
            clear_chat_history()  # å½“å‰queryçš„é¦–æ¬¡è°ƒç”¨ï¼Œè‡ªç„¶æ¸…ç©ºä¸Šä¸€æ¬¡çš„èŠå¤©è®°å½•
            return_prefix = ""
            for i in range(1, chatgpt_max_num, 1):  # retryå¤–åŒ…çŠ¶æ€ç çš„é—®é¢˜ï¼Œè¿™é‡Œè§£å†³ç©ºå€¼å’Œä¸å«ä»£ç çš„é—®é¢˜
                write_to_file(check_chatgpt_file,
                            f"ç¬¬{i}æ¬¡è°ƒç”¨chatgpt ç”¨äºç”Ÿæˆprefix\n")
                return_prefix = call_chatgpt(prefix_gen_prompt)
                # time.sleep(6)
                if return_prefix != "":  # å¦‚æœè°ƒç”¨è¿”å›æ­£å¸¸ï¼Œå°±è·³å‡ºå¾ªç¯
                    break
            return_prefix = form_complete_statement(return_prefix)
            write_to_file(check_chatgpt_file,
                        f"ç”Ÿæˆçš„prefixä¸ºï¼š{return_prefix}  \n"+'-'*100 + '\n')
            
            # å°†prefixå°è£…è¿›test
            test_with_only_prefix = encapsulate_into_test(
                return_prefix, tmp_query.testname)
            write_to_file(check_chatgpt_file,
                        f"å°è£…åçš„test_with_only_prefixä¸ºï¼š\n{test_with_only_prefix}  \n" + '-'*100 + '\n')

            test_with_only_prefix = test_with_only_prefix.strip()
            # è‡³æ­¤æ‹¿åˆ°äº† public void testGetTriggers() { final Argument arg = buildTargetsArgument(); }
            # oracleç”Ÿæˆ
            if test_with_only_prefix != "":
                query_test_method = test_with_only_prefix[:-1] + \
                    ORACLE_PLACEHOLDER + "\n" + test_with_only_prefix[-1]
            else:
                query_test_method = "public void " + \
                    tmp_query.testname + "{ " + ORACLE_PLACEHOLDER + "}"
            query_datapoint = Oracle_datapoint(
                query_test_method, tmp_query.focalname_paralist, tmp_query.testname, "")

            oracle_demopool = Oracle_Dataset(
                oracle_demopool_dir_path,
                "test_method.txt",
                "focalname_paralist.txt",
                "test_name.txt",
                "oracle.txt"
            )
            oracle_demopool_list = oracle_demopool.demopool_parse()

            # è¿™é‡Œå†³å®šç”¨å“ªç§æ£€ç´¢å™¨,too
            if selector == "unixcoder":
                oralce_candidate_demos, demo_sim_list = oracle_diversity_retriever(
                    query_datapoint, oracle_demopool_list, shot_num)
                if sample_order == "ascending":
                    sorted_oralce_candidate_demos = [
                        tu[1] for tu in sorted(
                            enumerate(oralce_candidate_demos), key=lambda tu:demo_sim_list[tu[0]], reverse=False
                        )
                    ]
                elif sample_order == "descending":
                    sorted_oralce_candidate_demos = [
                        tu[1] for tu in sorted(
                            enumerate(oralce_candidate_demos), key=lambda tu:demo_sim_list[tu[0]], reverse=True
                        )
                    ]
            elif selector == "real_random":
                oralce_candidate_demos = random.sample(
                    oracle_demopool_list, shot_num)

            # if selector == "bm25":
            #     oralce_candidate_demos = bm25_retrived_demos4oracle(
            #         query_datapoint, oracle_demopool_list, sample_order, shot_num)
            if sample_order == "ascending" or sample_order == "descending":
                oracle_gen_prompt = get_oracle_prompt(
                    sorted_oralce_candidate_demos, query_datapoint)
            else:
                oracle_gen_prompt = get_oracle_prompt(
                    oralce_candidate_demos, query_datapoint)

            return_oracle = ""
            # write_to_file(chat_history_file, ">>3<< \n")
            clear_chat_history()  # ç”Ÿæˆoracle ç¬¬ä¸‰ä¸ªæ¸…ç©ºèŠå¤©è®°å½•èŠ‚ç‚¹
            for i in range(1, chatgpt_max_num, 1):  # retryå¤–åŒ…çŠ¶æ€ç çš„é—®é¢˜ï¼Œè¿™é‡Œè§£å†³ç©ºå€¼å’Œä¸å«ä»£ç çš„é—®é¢˜
                write_to_file(check_chatgpt_file,
                            f"ç¬¬{i}æ¬¡è°ƒç”¨chatgpt ç”¨äºç”Ÿæˆoralce\n")
                return_oracle = call_chatgpt(oracle_gen_prompt)
                # time.sleep(6)
                if return_oracle != "":  # å¦‚æœè°ƒç”¨è¿”å›æ­£å¸¸ï¼Œå°±è·³å‡ºå¾ªç¯
                    break
            return_oracle = form_complete_statement(return_oracle)
            write_to_file(check_chatgpt_file,
                        f"ç”Ÿæˆçš„oralceä¸ºï¼š\n{return_oracle}  \n"+'-'*100 + '\n')
            test_unit = query_test_method.replace(
                ORACLE_PLACEHOLDER, return_oracle)

            # print("è‡³æ­¤ä¸€ä¸ªå®Œæ•´çš„æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆäº†:\n", test_unit)
            # print('-'*100)

            # å¯¹test_unitè¿›è¡Œç¼–è¯‘ï¼Œå¦‚æœtestunit_compile_numæ¬¡éƒ½ä¸é€šè¿‡å°±ä¸­æ–­è¿™ä¸€æ¬¡çš„ç”Ÿæˆ
            # write_to_file(chat_history_file, ">>4<< \n")
            clear_chat_history()  # ä¿®å¤test_unitçš„ç¼–è¯‘é”™è¯¯ ç¬¬4ä¸ªæ¸…ç©ºèŠå¤©è®°å½•èŠ‚ç‚¹
            compile_success = False
            for turn in range(1, testunit_compile_num+1, 1):
                compile_res, compile_out = compile_mytest(
                    pro, test_unit, id, testunit_compile_stderr_dir_path)
                result_datapoint.test_unit_compile_time += 1
                if compile_res:
                    logger.info(f"test unit åœ¨ç¬¬{turn}è½®ç¼–è¯‘æˆåŠŸ")
                    compile_success = True
                    break
                else:
                    if turn != testunit_compile_num:  # è‡³å¤šä¿®å¤testunit_compile_num-1æ¬¡
                        logger.info(f"test unit åœ¨ç¬¬{turn}è½®ç¼–è¯‘å¤±è´¥ï¼Œè¿›è¡Œä¿®å¤")
                        test_unit = repair_bf(
                            test_unit, compile_out, tmp_query, chatgpt_max_num)  # è¿™é‡Œä¸å¯¹ï¼Œç¬¬ä¸‰ä¸ªå‚æ•°æ˜¯Prefix_datapointï¼Œä¸æ˜¯Oracle_datapoint, åº”æƒ³åŠæ³•è§£å†³ï¼Œæˆ–è€…å¹²è„†ä¼ å…¥Prefix_datapoint?
                        write_to_file(
                            check_chatgpt_file, f"å®Œæˆç¬¬{turn}æ¬¡ä¿®å¤ï¼Œä¿®å¤åçš„test_unitä¸ºï¼š\n{test_unit}\n" + '-'*100 + '\n')
            if not compile_success:
                logger.info(f"test unit {testunit_compile_num}æ¬¡ç¼–è¯‘å‡å¤±è´¥ï¼Œæ”¾å¼ƒè¿™ä¸€æ¬¡çš„ç”Ÿæˆ")
                result_datapoint.test_unit_status = Test_Unit_Status.NOT_COMPILE_SUCCESS
                result_datapoint_list.append(result_datapoint)
                continue

            # ç¼–è¯‘é€šè¿‡åï¼Œæ‰§è¡Œtest_unitï¼Œå¦‚æœtestunit_execute_numæ¬¡éƒ½ä¸é€šè¿‡ï¼Œä¹Ÿå°±ä½œä¸ºæœ€åç»“æœ
            # write_to_file(chat_history_file, ">>5<< \n")
            clear_chat_history()  # ä¿®å¤test_unitçš„æ‰§è¡Œé”™è¯¯ ç¬¬5ä¸ª(also the last one)æ¸…ç©ºèŠå¤©è®°å½•èŠ‚ç‚¹
            execute_success = False
            for cycle in range(1, testunit_execute_num+1, 1):
                execute_res, execute_out = execute_mytest(
                    pro, test_unit, id, testunit_execute_stdout_dir_path)
                result_datapoint.test_unit_execute_time += 1
                if execute_res:
                    logger.info(f"test unit åœ¨ç¬¬{cycle}è½®æ‰§è¡ŒæˆåŠŸ")
                    execute_success = True
                    break
                else:
                    if cycle != testunit_execute_num:  # è‡³å¤šä¿®å¤testunit_execute_num-1æ¬¡
                        logger.info(f"test unit åœ¨ç¬¬{cycle}è½®æ‰§è¡Œå¤±è´¥ï¼Œè¿›è¡Œä¿®å¤")
                        test_unit = repair_tf_test_unit(
                            test_unit, execute_out, tmp_query, chatgpt_max_num)
                        write_to_file(
                            check_chatgpt_file, f"å®Œæˆç¬¬{cycle}æ¬¡ä¿®å¤ï¼Œä¿®å¤åçš„test_unitä¸ºï¼š\n{test_unit}\n"+'-'*100 + '\n')
            if not execute_success:
                logger.info(f"test unit {testunit_execute_num}æ¬¡æ‰§è¡Œå‡å¤±è´¥ï¼Œæ”¾å¼ƒè¿™ä¸€æ¬¡çš„ç”Ÿæˆ")
                logger.info(f"æœ€ç»ˆæ²¡æœ‰ç”Ÿæˆå¯ä»¥æ‰§è¡Œé€šè¿‡çš„ç”¨ä¾‹"+'\n'+'-'*100+'\n')
                result_datapoint.test_unit_status = Test_Unit_Status.COMPILE_SUCCESS_BUT_NOT_EXECUTE
            else:
                print(f"æµ‹è¯•é€šè¿‡~~~"+'\n'+'-'*100+'\n')
                test_unit = test_unit.replace("\n", " ")
                result_datapoint.passed_test_unit = test_unit
                result_datapoint.test_unit_status = Test_Unit_Status.EXECUTE_SUCCESS

            write_to_file(check_chatgpt_file, "#"*100+"\n")
            write_to_file(overall_result_file, result_datapoint.toString())

            status = result_datapoint.test_unit_status
            # times = int(id_to_num_list[now_id-1])
            if status == Test_Unit_Status.NOT_COMPILE_SUCCESS:
                bf_num += 1 # times
                write_to_file(passed_unit_file, str(result_datapoint.id)+", \n")
            elif status == Test_Unit_Status.COMPILE_SUCCESS_BUT_NOT_EXECUTE:
                tf_num += 1 # times
                write_to_file(passed_unit_file, str(result_datapoint.id)+", \n")
            elif status == Test_Unit_Status.EXECUTE_SUCCESS:
                passed_num += 1  # times
                write_to_file(passed_unit_file,
                            str(result_datapoint.id)+", "+result_datapoint.passed_test_unit+"\n")
            else:
                raise Exception("å‡ºç°äº†ä¸åº”è¯¥å‡ºç°çš„ç»“æœçŠ¶æ€")
        except Exception as e:
            logger.info(f"main got an exception: {e}")
            logger.info(f"ç»§ç»­æ‰§è¡Œç¬¬{id+1}ä¸ªè¾“å…¥")
            aborted_num += 1
            continue
    ''' å¯¹å®Œæ•´é¡¹ç›®çš„ç”Ÿæˆç»“æœè¿›è¡Œç»Ÿè®¡å’Œä¿å­˜,ç»“æœå¯èƒ½è½å…¥è¿™ä¹ˆå‡ ä¸ªåŒºé—´
    3ä¸ªå¤§ç±»åŒºé—´
    1. ç¼–è¯‘å¤±è´¥
    2. ç¼–è¯‘æˆåŠŸï¼Œæ‰§è¡Œå¤±è´¥
    3. ç¼–è¯‘æˆåŠŸï¼Œæ‰§è¡ŒæˆåŠŸ
    å…¶ä¸­æ¯ä¸ªå¤§ç±»åŒºé—´åˆç»†åˆ†ä¸º
    1. ç¼–è¯‘å¤±è´¥
    1.1 test_with_only_prefixç¼–è¯‘å¤±è´¥ï¼Œæ²¡æœ‰è¿›è¡Œoracleç”Ÿæˆ
    1.2 test_with_only_prefixç¼–è¯‘æˆåŠŸï¼Œè¿›è¡Œäº†oracleç”Ÿæˆï¼Œä½†æ˜¯test_unitç¼–è¯‘å¤±è´¥
    2. ç¼–è¯‘æˆåŠŸï¼Œæ‰§è¡Œå¤±è´¥
    test_with_only_prefixå’Œtest_unitå„ç¼–è¯‘å¤±è´¥å‡ æ¬¡
    3. ç¼–è¯‘æˆåŠŸï¼Œæ‰§è¡ŒæˆåŠŸ
    test_with_only_prefixå’Œtest_unitå„ç¼–è¯‘å¤±è´¥å‡ æ¬¡ï¼Œtest_unitæ‰§è¡Œå¤±è´¥å‡ æ¬¡

    '''
    # for result_datapoint in result_datapoint_list:
    #     write_to_file(overall_result_file, result_datapoint.toString())

    # ç°åœ¨æ˜¯éœ€è¦å¡«è®ºæ–‡è¡¨æ ¼çš„ç»“æœ
    # with open(id_to_num_file, "r") as f:
    #     id_to_num_list = f.readlines()
    # bf_num, tf_num, passed_num = 0, 0, 0
    # for result_datapoint in result_datapoint_list:
    #     now_id = result_datapoint.id
    #     status = result_datapoint.test_unit_status
    #     # times = int(id_to_num_list[now_id-1])
    #     if status == Test_Unit_Status.NOT_COMPILE_SUCCESS:
    #         bf_num += 1 # times
    #         write_to_file(passed_unit_file, str(result_datapoint.id)+", \n")
    #     elif status == Test_Unit_Status.COMPILE_SUCCESS_BUT_NOT_EXECUTE:
    #         tf_num += 1 # times
    #         write_to_file(passed_unit_file, str(result_datapoint.id)+", \n")
    #     elif status == Test_Unit_Status.EXECUTE_SUCCESS:
    #         passed_num += 1  # times
    #         write_to_file(passed_unit_file,
    #                     str(result_datapoint.id)+", "+result_datapoint.passed_test_unit+"\n")
    #     else:
    #         raise Exception("å‡ºç°äº†ä¸åº”è¯¥å‡ºç°çš„ç»“æœçŠ¶æ€")

    # æœ€åæŠŠæ•´ä½“çš„æƒ…å†µå†™å…¥åˆ°overall_outputä¸­
    # æ‰“å°æ•´ä¸ªé¡¹ç›®çš„è¿è¡Œç»“æœ
    total_num = bf_num + tf_num + passed_num

    # è¾“å‡ºæ¯ç§ç±»å‹çš„ç»å¯¹æ•°é‡å’Œæ‰€å æ¯”ä¾‹
    percentage = (aborted_num / total_num) * 100
    write_to_file(overall_result_file,
                  f"\n\naborted : {aborted_num} ({percentage:.2f}%) \n")
    percentage = (bf_num / total_num) * 100
    write_to_file(overall_result_file,
                f"build failed : {bf_num} ({percentage:.2f}%) \n")
    percentage = (tf_num / total_num) * 100
    write_to_file(overall_result_file,
                f"test failed : {tf_num} ({percentage:.2f}%) \n")
    percentage = (passed_num / total_num) * 100
    write_to_file(overall_result_file,
                f"passed : {passed_num} ({percentage:.2f}%) \n")
    
    print(f"åˆšåˆšç»“æŸçš„æ˜¯:{pro} from {start_idx} to {end_idx}")


if __name__ == "__main__":
    start_time = time.time()
    main()
    end_time = time.time()
    elapsed_time = end_time - start_time
    minutes = int(elapsed_time // 60)
    seconds = int(elapsed_time % 60)
    
    print(f"æ€»è€—æ—¶: {minutes} min {seconds} sec")
 