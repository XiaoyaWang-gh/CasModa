'''
python -m use_unixcoder
'''
from typing import List
import os
import sys
print(sys.executable)
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(project_root)

import warnings
from transformers import pipeline
from sklearn.cluster import KMeans
from scipy.spatial.distance import cosine
import torch
import numpy as np
from dotenv import load_dotenv

from CUTE_components.models import Prefix_datapoint, Oracle_datapoint, Testcase_datapoint
from CUTE_components.dataset import Prefix_Dataset, Oracle_Dataset, Testcase_Dataset


# è·å–å½“å‰è„šæœ¬çš„ç»å¯¹è·¯å¾„
script_dir = os.path.dirname(os.path.abspath(__file__))
# æ‹¼æ¥.envæ–‡ä»¶çš„è·¯å¾„
env_path = os.path.join(script_dir, '..', '.env')
# åŠ è½½.envæ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
load_dotenv(env_path)
# è·å–å½“å‰æµ‹è¯•çš„é¡¹ç›®
pro = os.getenv("DEMO_PRO")


def encode_by_unixcoder(input_str):
    '''
    ç”¨unixcoderç¼–ç å­—ç¬¦ä¸²ï¼Œè¿”å›Tensor (1,768)
    '''
    local_path = os.path.join(os.path.dirname(__file__), "localunixcoder")
    extractor = pipeline("feature-extraction", model=local_path)
    output_tensor = extractor(input_str, return_tensors=True)
    # before torch.Size([1, 38, 768])
    final_output = torch.mean(output_tensor, dim=1)  # ç¬¬äºŒä¸ªç»´åº¦æ˜¯åºåˆ—é•¿åº¦ï¼Œä¸ç¡®å®šç›´æ¥åˆ é™¤å¯¹ä¸å¯¹
    # after torch.Size([1, 768])
    assert final_output.shape == torch.Size([1, 768])

    return final_output


def cosine_similarity(ts1, ts2):
    '''
    ts1 : Tensor (1,768)
    ts2 : Tensor (1,768)
    è®¡ç®—æ¯ä¸ªqueryå‘é‡å’Œdemo_poolä¸­å‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦
    '''
    vector1 = ts1.flatten().numpy()
    vector2 = ts2.flatten().numpy()
    similarity = 1 - cosine(vector1, vector2)

    return similarity  # å–å€¼èŒƒå›´æ˜¯[-1,1],è¶Šå¤§è¶Šç›¸ä¼¼


def most_alike_by_cossimi(query_ts, ts_list):
    '''
    query_ts : Tensor (1,768)
    ts_list : List[(int,str,Tensor)]
    é€šè¿‡ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œä»ts_listä¸­é€‰å‡ºå’Œquery_tsæœ€ç›¸åƒçš„ts,è¿”å›å…¶index(å³å…ƒç»„çš„ç¬¬ä¸€ä¸ªå…ƒç´ )
    return: int, float
    '''
    max_similarity = -1
    max_similarity_idx = -1
    for i in range(len(ts_list)):
        idx = ts_list[i]["idx"]
        ts = ts_list[i]["tensor"]
        similarity = cosine_similarity(query_ts, ts)
        if similarity > max_similarity:
            max_similarity = similarity
            max_similarity_idx = idx
    return max_similarity_idx, max_similarity


def sort_by_cossimi(query, samples, reverse=False):
    '''
    query : str
    samples : list(str)  [0.68, 0.26, 0.77, 0.80, 0.25]
    return : list(int)   [2, 1, 3, 4, 0]
    åˆ©ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œå°†samplesä¸­çš„å…ƒç´ æŒ‰ç…§å’Œqueryç›¸ä¼¼åº¦å‡åº/é™åºæ’åº
    è¿”å›ç´¢å¼•åˆ—è¡¨
    '''
    query_tensor = encode_by_unixcoder(query)
    sample_tensors = [encode_by_unixcoder(i) for i in samples]
    similarity_list = [cosine_similarity(
        query_tensor, sample_tensor) for sample_tensor in sample_tensors]
    # ä½¿ç”¨sorted()å‡½æ•°å¯¹æ•°ç»„å…ƒç´ è¿›è¡Œå‡åºæ’åˆ—ï¼Œå¹¶è¿”å›æ¯ä¸ªå…ƒç´ çš„åºå·
    idx_list = [x for x in range(len(similarity_list))]
    sorted_idx_list = sorted(
        idx_list, key=lambda x: similarity_list[x], reverse=reverse)
    return sorted_idx_list


def prefix_sort_by_cossimi(query_dp, samples, reverse=False):
    '''
    query_dp : Prefix_datapoint
    samples : list(Prefix_datapoint)
    return : list(int)
    åˆ©ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œå°†samplesä¸­çš„å…ƒç´ æŒ‰ç…§å’Œquery_dpç›¸ä¼¼åº¦ä»å¤§åˆ°å°æ’åº
    è¿™ä¸ªæ–¹æ³•æœ¬è´¨ä¸Šåªæ˜¯æŠŠå­—ç¬¦ä¸²æå–å‡ºæ¥ï¼Œæ’åºè¿˜æ˜¯é sort_by_cossimi
    '''
    """  
    **Deprecated**: This method is no longer recommended for use.  
    """
    warnings.warn(
        "This method is deprecated and will be removed in future versions.", DeprecationWarning)

    assert isinstance(query_dp, Prefix_datapoint)
    assert all(isinstance(sample, Prefix_datapoint) for sample in samples)

    query_str = query_dp.classname+query_dp.constructor+query_dp.focalname_paralist
    sample_strs = [
        sample.classname+sample.constructor+sample.focalname_paralist for sample in samples]

    idx_list = sort_by_cossimi(query_str, sample_strs, reverse=reverse)

    return idx_list


def oracle_sort_by_cossimi(query_dp, samples, reverse=False):
    '''
    query_dp : Oracle_datapoint
    samples : list(Oracle_datapoint)
    return : list(int)
    åˆ©ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œå°†samplesä¸­çš„å…ƒç´ æŒ‰ç…§å’Œquery_dpç›¸ä¼¼åº¦ä»å¤§åˆ°å°æ’åº
    è¿™ä¸ªæ–¹æ³•æœ¬è´¨ä¸Šåªæ˜¯æŠŠå­—ç¬¦ä¸²æå–å‡ºæ¥ï¼Œæ’åºè¿˜æ˜¯é sort_by_cossimi
    '''
    """  
    **Deprecated**: This method is no longer recommended for use.  
    """
    warnings.warn(
        "This method is deprecated and will be removed in future versions.", DeprecationWarning)

    assert isinstance(query_dp, Oracle_datapoint)
    assert all(isinstance(sample, Oracle_datapoint) for sample in samples)

    query_str = query_dp.focalname_paralist+query_dp.test_method
    sample_strs = [
        sample.focalname_paralist+sample.test_method for sample in samples]

    idx_list = sort_by_cossimi(query_str, sample_strs, reverse=reverse)

    return idx_list


def cluster_by_kmeans(tensor_list, cluster_num):
    '''
    tensor_list ï¼š list of Tensor (1,768)
    cluster_num : int
    ç”¨kmeansèšç±»æˆkä¸ªç°‡ï¼Œè¿”å›kç»´list
    '''

    # å°†tensor_listè½¬åŒ–ä¸ºåˆæ³•è¾“å…¥
    concatenated_tensor = torch.cat(tensor_list, dim=0)
    numpy_array = concatenated_tensor.numpy()
    print("numpy_array.shape:\n", numpy_array.shape)

    # æ‰§è¡Œèšç±»
    kmeans = KMeans(n_clusters=cluster_num)
    labels = kmeans.fit_predict(numpy_array)  # è¾“å‡ºç±»ä¼¼äº[4 4 4 2 2 2]ï¼Œè¡¨ç¤ºæ¯ä¸ªæ ·æœ¬å±äºå“ªä¸ªç°‡
    label_list = labels.tolist()  # è½¬åŒ–ä¸ºintç±»å‹çš„list

    return label_list


def read_unixcoder(tensor_file_path):
    '''
    ä»æ–‡ä»¶é‡Œé¢è¯»å‘é‡è¿˜æŒºç¹ççš„ï¼Œåœ¨ä¸‰ä¸ªåœ°æ–¹ç”¨åˆ°ï¼Œæ‰€ä»¥å•ç‹¬å°è£…ä¸€ä¸‹
    : param file_path : str
    : return : list[tensor]
    '''
    encoded_list = []
    with open(tensor_file_path, "r", encoding="utf-8") as f:
        for line in f.readlines():
            # å¯¹åŸå§‹å­—ç¬¦ä¸²[-0.03974078595638275, 0.441280335187912, -1.3046798706054688] è¿›è¡Œå¤„ç†
            line = line.strip()
            line = line[1:-1]
            num_list = line.split(",")
            value_list = [float(num) for num in num_list]
            # è½¬åŒ–ä¸ºtensor
            tensor = torch.tensor(value_list).view(1, 768)
            encoded_list.append(tensor)
    return encoded_list


def prefix_diversity_retriever(query_tensor, demo_pool, shot_num):
    '''
    è¦ç´ é½å…¨ï¼šunixcoderç¼–ç ï¼Œèšç±»ï¼Œä½™å¼¦ç›¸ä¼¼åº¦é€‰å‡ºæ¯ä¸ªç°‡ä¸­æœ€ç›¸åƒçš„ï¼Œå­˜å…¥listè¿”å›
    @param
    query_tensor: Prefix_datapoint ç»è¿‡ç¼–ç åçš„å‘é‡
    demo_pool: List[Prefix_datapoint]
    order: str
    shot_num: int
    @return
    candidates: List[Prefix_datapoint]
    demo_similarity_list: List[float]
    '''
    candidates: List[Prefix_datapoint] = []

    # 1. unixcoderç¼–ç 
    encoded_demo_pool = []
    # ç›´æ¥è¯»å–ç¼–ç å¥½çš„
    tensor_file = f"txt_repo\\prefix\\demo_pool\\{pro}\\unixcoder_tensor.txt"
    encoded_demo_pool = read_unixcoder(tensor_file)

    # 2. èšç±»
    label_list = cluster_by_kmeans(encoded_demo_pool, shot_num)
    # æ ¹æ®æ ‡ç­¾å½’ç±»
    clustered_2d_list = [[] for _ in range(shot_num)]

    if len(demo_pool) != len(label_list):
        print("prefix_diversity_retriever >> Length mismatch in PREFIX STAGE!!")
        print("Length of demo_pool:", len(demo_pool))
        print("Length of label_list:", len(label_list))
        exit()  # ç»ˆæ­¢ç¨‹åº

    for i in range(len(label_list)):
        label = label_list[i]
        demo = demo_pool[i]
        # å­˜å…¥å…ƒç»„æ˜¯ä¸ºäº†è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦å¥½è°ƒåº¦index
        tmp_dict = dict(idx=i, demo=demo, tensor=encoded_demo_pool[i])
        clustered_2d_list[label].append(tmp_dict)
    # 3. é€šè¿‡ä½™å¼¦ç›¸ä¼¼åº¦é€‰å‡ºæ¯ä¸ªç°‡ä¸­æœ€ç›¸åƒçš„å…ƒç»„çš„index

    candidate_idxs = []  # æœ€åè¦è¿”å›çš„demoçš„ç´¢å¼•çš„list
    demo_similarity_list = []  # å‡ºå»ä»¥åæ’åºè¦ç”¨
    for clustered_1d_list in clustered_2d_list:
        most_alike_demo_idx, max_similarity = most_alike_by_cossimi(
            query_tensor, clustered_1d_list)
        candidate_idxs.append(most_alike_demo_idx)
        demo_similarity_list.append(max_similarity)
    # 4. å­˜å…¥listè¿”å›
    candidates = [demo_pool[idx] for idx in candidate_idxs]

    return candidates, demo_similarity_list


def oracle_diversity_retriever(query, demo_pool, shot_num):
    '''
    è¦ç´ é½å…¨ï¼šunixcoderç¼–ç ï¼Œèšç±»ï¼Œä½™å¼¦ç›¸ä¼¼åº¦é€‰å‡ºæ¯ä¸ªç°‡ä¸­æœ€ç›¸åƒçš„ï¼Œå­˜å…¥listè¿”å›
    @param
    query: Prefix_datapoint ç»è¿‡ç¼–ç åçš„å‘é‡
    demo_pool: List[Oracle_datapoint]
    order: str
    shot_num: int
    @return
    candidates: List[Oracle_datapoint]
    '''
    candidates: List[Oracle_datapoint] = []
    # 1. unixcoderç¼–ç 
    encoded_demo_pool = []
    # ç›´æ¥è¯»å–ç¼–ç å¥½çš„
    tensor_file = f"txt_repo\\oracle\\demo_pool\\{pro}\\unixcoder_tensor.txt"
    encoded_demo_pool = read_unixcoder(tensor_file)

    # 2. èšç±»
    label_list = cluster_by_kmeans(encoded_demo_pool, shot_num)
    # æ ¹æ®æ ‡ç­¾å½’ç±»
    clustered_2d_list = [[] for _ in range(shot_num)]

    if len(demo_pool) != len(label_list):
        print("oracle_diversity_retriever >> Length mismatch in ORACLE STAGE!!")
        print("Length of demo_pool:", len(demo_pool))
        print("Length of label_list:", len(label_list))
        exit()  # ç»ˆæ­¢ç¨‹åº

    for i in range(len(label_list)):
        label = label_list[i]
        demo = demo_pool[i]
        # å­˜å…¥å…ƒç»„æ˜¯ä¸ºäº†è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦å¥½è°ƒåº¦index
        tmp_dict = dict(idx=i, demo=demo, tensor=encoded_demo_pool[i])
        clustered_2d_list[label].append(tmp_dict)
    # 3. é€šè¿‡ä½™å¼¦ç›¸ä¼¼åº¦é€‰å‡ºæ¯ä¸ªç°‡ä¸­æœ€ç›¸åƒçš„å…ƒç»„çš„index
    query_tensor = encode_by_unixcoder(
        query.focalname_paralist+query.test_method)
    candidate_idxs = []  # æœ€åè¦è¿”å›çš„demoçš„ç´¢å¼•çš„list
    demo_similarity_list = []  # å‡ºå»ä»¥åæ’åºè¦ç”¨
    for clustered_1d_list in clustered_2d_list:
        most_alike_demo_idx, max_similarity = most_alike_by_cossimi(
            query_tensor, clustered_1d_list)
        candidate_idxs.append(most_alike_demo_idx)
        demo_similarity_list.append(max_similarity)
    # 4. å­˜å…¥listè¿”å›
    candidates = [demo_pool[idx] for idx in candidate_idxs]

    return candidates, demo_similarity_list


def testcase_diversity_retriever(query, demo_pool, shot_num):
    '''
    å’Œä¸Šé¢ä¸¤ä¸ªæ–¹æ³•åŠŸèƒ½ç±»ä¼¼ï¼Œæœ€å¤§çš„åŒºåˆ«åœ¨äºä¼ å…¥çš„Testcase_datapointè‡ªèº«å°±æœ‰å‘é‡unix_tensor

    '''
    candidates: List[Testcase_datapoint] = []
    # 1. unixcoderç¼–ç 
    encoded_demo_pool = [demo.unix_tensor for demo in demo_pool]

    # 2. èšç±»
    label_list = cluster_by_kmeans(encoded_demo_pool, shot_num)
    # æ ¹æ®æ ‡ç­¾å½’ç±»
    clustered_2d_list = [[] for _ in range(shot_num)]

    if len(demo_pool) != len(label_list):
        print("Length mismatch in PREFIX STAGE!!")
        print("Length of demo_pool:", len(demo_pool))
        print("Length of label_list:", len(label_list))
        exit()  # ç»ˆæ­¢ç¨‹åº

    for i in range(len(label_list)):
        label = label_list[i]
        demo = demo_pool[i]
        # å­˜å…¥å…ƒç»„æ˜¯ä¸ºäº†è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦å¥½è°ƒåº¦index
        tmp_dict = dict(idx=i, demo=demo, tensor=encoded_demo_pool[i])
        clustered_2d_list[label].append(tmp_dict)
    # 3. é€šè¿‡ä½™å¼¦ç›¸ä¼¼åº¦é€‰å‡ºæ¯ä¸ªç°‡ä¸­æœ€ç›¸åƒçš„å…ƒç»„çš„index
    query_tensor = query.unix_tensor
    candidate_idxs = []  # æœ€åè¦è¿”å›çš„demoçš„ç´¢å¼•çš„list
    demo_similarity_list = []  # å‡ºå»ä»¥åæ’åºè¦ç”¨
    for clustered_1d_list in clustered_2d_list:
        most_alike_demo_idx, max_similarity = most_alike_by_cossimi(
            query_tensor, clustered_1d_list)
        candidate_idxs.append(most_alike_demo_idx)
        demo_similarity_list.append(max_similarity)
    # 4. å­˜å…¥listè¿”å›
    candidates = [demo_pool[idx] for idx in candidate_idxs]

    return candidates, demo_similarity_list


def main():


    ut_file = ""
    dataset_tobe_encoded = []
    dir_path = r'/Users/bytedance/code/casmodatest/CasModa/txt_repo/testbody/demo_pool/ecommerce'

    ut_file = os.path.join(
        dir_path, "fm_tc_tensor.txt")

    dataset_tobe_encoded = Testcase_Dataset(
        dir_path,
        "classname.txt",
        "constr_sign.txt",
        "focalname_paralist.txt",
        "test_name.txt",
        "fake_test_body.txt",
        "unixcoder_tensor.txt"
    ).parse()

    with open(ut_file, "a", encoding="utf-8") as f:
        cnt = 0
        for dp in dataset_tobe_encoded:
            ut = encode_by_unixcoder(
                dp.classname+dp.constructor+dp.focalname_paralist)  # testbody

            # turn Tensor to list
            ut_str = str(ut.flatten().tolist())
            f.write(ut_str+"\n")
            cnt += 1
            print(f"è¿™æ˜¯ç¬¬{cnt}ä¸ªï¼Œè¿˜å·®{len(dataset_tobe_encoded)-cnt}ä¸ªæœªç¼–ç ğŸ”†")


def encode_ecommerce():
    dir_path = r'/Users/bytedance/code/casmodatest/CasModa/txt_repo/testbody/demo_pool/ecommerce'

    dest_file = os.path.join(dir_path, "fm_tc_tensor.txt")
    fm_input_file = os.path.join(dir_path, "focal_method.txt")
    tc_input_file = os.path.join(dir_path, "test_case.txt")
    with open(dest_file, "a", encoding="utf-8") as f:
        with open(fm_input_file, "r", encoding="utf-8") as fm_f:
            fm_list = fm_f.readlines()
        with open(tc_input_file, "r", encoding="utf-8") as tc_f:
            tc_list = tc_f.readlines()
    assert len(fm_list) == len(tc_list)

    for i in range(len(fm_list)):
        cnt = i+1
        fm = fm_list[i].strip()
        tc = tc_list[i].strip()
        ut = encode_by_unixcoder(fm+"  "+tc)  # testbody
        # turn Tensor to list
        ut_str = str(ut.flatten().tolist())
        f.write(ut_str+"\n")
        print(f"è¿™æ˜¯ç¬¬{cnt}ä¸ªï¼Œè¿˜å·®{len(fm_list) - cnt}ä¸ªæœªç¼–ç ğŸ”†")



if __name__ == "__main__":
    encode_by_unixcoder("test")
    # encode_ecommerce()
